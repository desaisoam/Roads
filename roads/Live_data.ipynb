{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mappable Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter an address and at the end, it will save a CSV as final_data.csv in the Data folder that can be used in a Tableau workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tweepy\n",
    "import jsonpickle\n",
    "import json\n",
    "import datetime\n",
    "import config\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from pandas.io.json import json_normalize\n",
    "import regex as re\n",
    "import pickle\n",
    "import json, requests\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import warnings; warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "address = '1901, 1825 Eastshore Hwy, CA'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter API Verification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = ''\n",
    "consumer_secret = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soam Desai\n"
     ]
    }
   ],
   "source": [
    "user = api.me()\n",
    "print (user.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address Search\n",
    "\n",
    "The below code searches for your address on mapdevelopers.com and returns both the geolocation coordinates and the upper left and lower right corners of a bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run selenium webdriver to search for address and return latitude and longitude (turn this into a function)\n",
    "def find_address(address):\n",
    "    #establish driver and URL\n",
    "    driver = webdriver.Chrome(executable_path='C:\\\\bin\\\\chromedriver')\n",
    "    url = 'https://www.mapdevelopers.com/geocode_tool.php'\n",
    "    driver.get(url)\n",
    "    time.sleep(10)\n",
    "    #fill in address and click button\n",
    "    address_field = driver.find_element_by_id(\"address\")\n",
    "    address_field.send_keys(address)\n",
    "    search_button = driver.find_element_by_xpath('//*[@id=\"search-form\"]/div[1]/span[2]/button')\n",
    "    search_button.click()\n",
    "    #set up \n",
    "    html= driver.page_source\n",
    "    time.sleep(10)\n",
    "    html= driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "    location = (soup.find('div', {'id':\"display_lat\"}).text, soup.find('div', {'id':\"display_lng\"}).text)    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latitude, longitude = find_address(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.2329699,-121.9452664\n",
      "37.5129699,-122.6652664\n"
     ]
    }
   ],
   "source": [
    "top_left = (str(float(latitude)+.36)+ ','+ str(float(longitude)+.36))\n",
    "print(top_left)\n",
    "bottom_right = (str(float(latitude)-.36) + ','+str(float(longitude)-.36))\n",
    "print(bottom_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for live tweets\n",
    "\n",
    "The below code pulls in tweets from the past 24 hours, and returns only tweets with geolocation data attached in a dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "year = datetime.datetime.now().year\n",
    "month = datetime.datetime.now().month\n",
    "day = datetime.datetime.now().day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tweets(search_terms = \"\"):\n",
    "    tweet_id = []\n",
    "    tweet_time = []\n",
    "    tweet_text = []\n",
    "    tweet_coordinates = []\n",
    "    tweet_place = []\n",
    "    tweet_count = 0\n",
    "    tweet_dataframe = pd.DataFrame()\n",
    "    for tweet in tweepy.Cursor(api.search,\n",
    "                               q=search_terms,\n",
    "                               geocode =f'{latitude},{longitude},25mi',\n",
    "                               since = f'{year}-{month}-{day-1}',\n",
    "                               rpp=100,\n",
    "                               result_type=\"recent\",\n",
    "                               include_entities=True,).items(1000):\n",
    "        if tweet.coordinates != None or tweet.place !=None: \n",
    "            tweet_id.append(tweet.id_str)\n",
    "            tweet_time.append(tweet.created_at)\n",
    "            tweet_text.append(tweet.text)\n",
    "            tweet_coordinates.append(tweet.coordinates)\n",
    "            tweet_place.append(tweet.place)\n",
    "            tweet_count += 1\n",
    "        else:\n",
    "            pass\n",
    "    tweet_dataframe['id'] = tweet_id\n",
    "    tweet_dataframe['time'] = tweet_time\n",
    "    tweet_dataframe['text'] = tweet_text\n",
    "    tweet_dataframe['coordinates'] =tweet_coordinates\n",
    "    tweet_dataframe['place'] = tweet_place\n",
    "    \n",
    "    print(tweet_count)\n",
    "    return tweet_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['road closed', \"road closure\", 'intersection blocked', \n",
    "                'blocked road', 'flooded road', \"closed due to road\",\n",
    "               'intersection closed', 'highway closed', 'hwy closed',\n",
    "               'street closed', 'street flooded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataframe = pd.DataFrame(columns = ['id', 'time', 'text', 'coordinates', 'place'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>time</th>\n",
       "      <th>text</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1359601049930240005</td>\n",
       "      <td>2021-02-10 20:32:05</td>\n",
       "      <td>Intersection of Essex St &amp;amp; Folsom St\\n\\nBl...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-122.3955847...</td>\n",
       "      <td>Place(_api=&lt;tweepy.api.API object at 0x0000023...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                time  \\\n",
       "0  1359601049930240005 2021-02-10 20:32:05   \n",
       "\n",
       "                                                text  \\\n",
       "0  Intersection of Essex St &amp; Folsom St\\n\\nBl...   \n",
       "\n",
       "                                         coordinates  \\\n",
       "0  {'type': 'Point', 'coordinates': [-122.3955847...   \n",
       "\n",
       "                                               place  \n",
       "0  Place(_api=<tweepy.api.API object at 0x0000023...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for term in search_terms:\n",
    "    df = get_tweets(term)\n",
    "    raw_dataframe = raw_dataframe.append(df)\n",
    "    \n",
    "raw_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Clean data and run through model\n",
    "data = raw_dataframe.drop_duplicates(subset='text')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean data according to final modeling notbook\n",
    "data = data.drop_duplicates(subset='text')\n",
    "data['text'] = [i.lower() for i in data['text']]\n",
    "data['text'] = [re.sub(r'@[A-z0-9]*', r' ', i) \n",
    "                    for i in data['text']];\n",
    "data['text'] = [re.sub('[^A-Za-z0-9#]+', ' ', i) \n",
    "                    for i in data['text']];\n",
    "lm = WordNetLemmatizer()\n",
    "data['text'] = [\" \".join([lm.lemmatize(w) for w in i.split()]) \n",
    "                   for i in data['text']];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import final model to read tweets using Pickle:\n",
    "rating_model = pickle.load(open('C:\\\\Users\\\\desai\\\\Desktop\\\\roads\\\\Project_Notebooks\\\\gb_model.sav', 'rb'))\n",
    "tvec = pickle.load(open('C:\\\\Users\\\\desai\\\\Desktop\\\\roads\\\\Project_Notebooks\\\\tvec.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_text = tvec.transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['ratings'] = rating_model.predict(transformed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "Name: ratings, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ratings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out any lane closures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list1 = ['road', 'street','rd', 'hwy', 'highway', 'ave', 'avenue','intersection']\n",
    "word_list2 = ['closed','closure', 'blocked', 'flooded']\n",
    "not_word_list = ['lane closed', 'lane closure','cleared', 're opened', 'reopen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_road_closure'] = data['text'].map(lambda x: 1 if ((any(word in x for word in word_list1))\n",
    "                                                           & (any(word in x for word in word_list2))\n",
    "                                                           & (not any(word in x for word in not_word_list))\n",
    "                                                          ) else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataframe for next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempt_to_map = data[(data['ratings']==1) & (data['is_road_closure'] == 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append Easily mappable tweets to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns without exact coordinates\n",
    "map_first = attempt_to_map.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out longitude and latitude, and modify data to be in final dataframe format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_first[ 'LONGITUDE'] = [point['coordinates'][0] for point in map_first['coordinates']]\n",
    "map_first['LATITUDE'] = [point['coordinates'][1] for point in map_first['coordinates']]\n",
    "map_first['start_end'] = ['Twitter' for i in map_first['ratings']]\n",
    "map_first['incident'] = map_first['time'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mapping_df = map_first[['LATITUDE', 'LONGITUDE', 'start_end', 'incident']]\n",
    "final_mapping_df.to_csv('C:\\\\Users\\\\desai\\\\Desktop\\\\roads\\\\data\\\\final_data5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse remaining tweets for mappable intersections\n",
    "\n",
    "As explained in our project documentation, we did not execute code parsing the tweets, as we were unable to generalize it to read tweets written by normal twitter users. For future use, the other tweets can be found in the below dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_second = attempt_to_map[attempt_to_map['coordinates'].isna()]\n",
    "map_second.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull HERE.com data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set app_id and code\n",
    "YOUR_APP_ID= ''\n",
    "YOUR_APP_CODE= ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://traffic.ls.hereapi.com/traffic/6.3/incidents.json?bbox=38.2329699,-121.9452664;37.5129699,-122.6652664&apiKey=v5C8Pz4ujlMFgUtqGucUkumZmB_pqkCJ3R3IKmIjl8M&criticality=critical\n"
     ]
    }
   ],
   "source": [
    "url = f'https://traffic.ls.hereapi.com/traffic/6.3/incidents.json?bbox={top_left};{bottom_right}&apiKey=ENTER HERE&criticality=critical'\n",
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'17:45'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = requests.get(url)\n",
    "print(res)\n",
    "date_requested = time.strftime('%Y-%m-%d')\n",
    "time_requested = time.strftime('%H:%M') # 24-hour format\n",
    "date_requested; time_requested\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using methods written out in the HERE.com data cleaning notebook, append the final_mapping_df with live traffic data streamed from HERE.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(568, 4)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmaps_json = res.json()\n",
    "incidents = pd.DataFrame(hmaps_json['TRAFFIC_ITEMS']['TRAFFIC_ITEM'])\n",
    "incidents = incidents.drop(axis=1, columns=['ABBREVIATION', 'ORIGINAL_TRAFFIC_ITEM_ID', 'PRODUCT',\n",
    "                                              'TRAFFIC_ITEM_ID', 'mid'])\n",
    "incidents.rename(str.lower, axis=1, inplace=True)\n",
    "locations = pd.DataFrame(list(incidents['location']))\n",
    "locations.rename(str.lower, axis=1, inplace=True)\n",
    "locations = locations.applymap(lambda x: {} if pd.isnull(x) else x)\n",
    "loc_coord = pd.DataFrame(list(locations['geoloc']))\n",
    "loc_coord.rename(str.lower, axis=1, inplace=True)\n",
    "origin = loc_coord['origin'].apply(pd.Series)\n",
    "origin['start_end'] = 'start'\n",
    "origin['incident'] = origin.index\n",
    "to = pd.DataFrame(s for k, v in loc_coord['to'].items() for s in v)\n",
    "to['start_end'] = 'end'\n",
    "to['incident'] = to.index\n",
    "all_locations = pd.concat((origin, to), axis=0)\n",
    "all_locations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mapping_df = final_mapping_df.append(all_locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export final data to CSV!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mapping_df.to_csv('C:\\\\Users\\\\desai\\\\Desktop\\\\roads\\\\data\\\\final_data9.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, map the road closures in Tableau"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
